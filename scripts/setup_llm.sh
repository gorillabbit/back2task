#!/bin/bash
# Back2Task LLM Server Setup Script
# OpenAI gpt-oss-20b ã‚’ vLLM ã§èµ·å‹•

set -e

# è‰²ä»˜ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}ðŸ¤– Back2Task LLM Server Setup${NC}"
echo "================================"

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
MODEL_NAME="openai/gpt-oss-20b"
PORT=8000
MAX_MODEL_LEN=32768
QUANTIZATION=""
DEVICE="auto"

# å¼•æ•°è§£æž
while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL_NAME="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --max-len)
            MAX_MODEL_LEN="$2"
            shift 2
            ;;
        --quantization)
            QUANTIZATION="$2"
            shift 2
            ;;
        --cpu)
            DEVICE="cpu"
            shift
            ;;
        --help)
            echo "Usage: $0 [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  --model MODEL_NAME     Model to use (default: openai/gpt-oss-20b)"
            echo "  --port PORT           Port to run on (default: 8000)"
            echo "  --max-len LENGTH      Max model length (default: 32768)"
            echo "  --quantization TYPE   Quantization type (awq, gptq, etc.)"
            echo "  --cpu                 Force CPU inference"
            echo "  --help                Show this help"
            exit 0
            ;;
        *)
            echo -e "${RED}Unknown option: $1${NC}"
            exit 1
            ;;
    esac
done

echo -e "${YELLOW}è¨­å®š:${NC}"
echo "  ãƒ¢ãƒ‡ãƒ«: $MODEL_NAME"
echo "  ãƒãƒ¼ãƒˆ: $PORT"
echo "  æœ€å¤§é•·: $MAX_MODEL_LEN"
echo "  ãƒ‡ãƒã‚¤ã‚¹: $DEVICE"
if [[ -n "$QUANTIZATION" ]]; then
    echo "  é‡å­åŒ–: $QUANTIZATION"
fi
echo ""

# Python ç’°å¢ƒãƒã‚§ãƒƒã‚¯
echo -e "${BLUE}ðŸ” ç’°å¢ƒãƒã‚§ãƒƒã‚¯${NC}"

if ! command -v python3 &> /dev/null; then
    echo -e "${RED}âŒ Python3 ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… Python3: $(python3 --version)${NC}"

# GPU ãƒã‚§ãƒƒã‚¯
if command -v nvidia-smi &> /dev/null; then
    echo -e "${GREEN}âœ… NVIDIA GPU æ¤œå‡º:${NC}"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | head -3
    GPU_AVAILABLE=true
else
    echo -e "${YELLOW}âš ï¸  NVIDIA GPU ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ${NC}"
    echo -e "${YELLOW}   CPUæŽ¨è«–ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆä½Žé€Ÿï¼‰${NC}"
    GPU_AVAILABLE=false
    DEVICE="cpu"
fi

# vLLM ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
VLLM_ENV="$HOME/venv/vllm"

echo -e "\n${BLUE}ðŸ› ï¸  vLLM ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—${NC}"

if [[ ! -d "$VLLM_ENV" ]]; then
    echo "vLLMç”¨Pythonç’°å¢ƒã‚’ä½œæˆä¸­..."
    python3 -m venv "$VLLM_ENV"
fi

echo "vLLMç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆä¸­..."
source "$VLLM_ENV/bin/activate"

echo "vLLMã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
pip install --upgrade pip

if [[ "$GPU_AVAILABLE" == "true" ]]; then
    # GPUç‰ˆ
    pip install vllm>=0.5.0
else
    # CPUç‰ˆ
    pip install vllm>=0.5.0 --extra-index-url https://download.pytorch.org/whl/cpu
fi

echo -e "${GREEN}âœ… vLLM ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†${NC}"

# ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèª
echo -e "\n${BLUE}ðŸ“¦ ãƒ¢ãƒ‡ãƒ«æº–å‚™${NC}"

python3 -c "
from huggingface_hub import snapshot_download
import os

model_name = '$MODEL_NAME'
print(f'ãƒ¢ãƒ‡ãƒ« {model_name} ã®å¯ç”¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...')

try:
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã—ãªã„ï¼‰
    from transformers import AutoConfig
    config = AutoConfig.from_pretrained(model_name)
    print(f'âœ… ãƒ¢ãƒ‡ãƒ« {model_name} ãŒåˆ©ç”¨å¯èƒ½ã§ã™')
    print(f'   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {config.num_parameters if hasattr(config, \"num_parameters\") else \"ä¸æ˜Ž\"}')
    print(f'   éš ã‚Œå±¤ã‚µã‚¤ã‚º: {config.hidden_size if hasattr(config, \"hidden_size\") else \"ä¸æ˜Ž\"}')
except Exception as e:
    print(f'âŒ ãƒ¢ãƒ‡ãƒ« {model_name} ã®å–å¾—ã«å¤±æ•—: {e}')
    print('åˆ©ç”¨å¯èƒ½ãªä»£æ›¿ãƒ¢ãƒ‡ãƒ«:')
    print('  - microsoft/DialoGPT-large')
    print('  - EleutherAI/gpt-neo-2.7B')
    print('  - EleutherAI/gpt-j-6b')
    exit(1)
"

if [[ $? -ne 0 ]]; then
    echo -e "${RED}âŒ ãƒ¢ãƒ‡ãƒ«æº–å‚™å¤±æ•—${NC}"
    exit 1
fi

# vLLM ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã‚³ãƒžãƒ³ãƒ‰æ§‹ç¯‰
VLLM_CMD="python -m vllm.entrypoints.openai.api_server"
VLLM_CMD="$VLLM_CMD --model $MODEL_NAME"
VLLM_CMD="$VLLM_CMD --served-model-name gpt-oss-20b"
VLLM_CMD="$VLLM_CMD --port $PORT"
VLLM_CMD="$VLLM_CMD --max-model-len $MAX_MODEL_LEN"

if [[ -n "$QUANTIZATION" ]]; then
    VLLM_CMD="$VLLM_CMD --quantization $QUANTIZATION"
fi

if [[ "$DEVICE" == "cpu" ]]; then
    VLLM_CMD="$VLLM_CMD --enforce-eager"
    # CPUæŽ¨è«–ã®å ´åˆã¯ä½Žã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æŽ¨å¥¨
    if [[ "$MAX_MODEL_LEN" -gt 16384 ]]; then
        echo -e "${YELLOW}âš ï¸  CPUæŽ¨è«–ã§ã¯ä½Žã„max-model-lenã‚’æŽ¨å¥¨ã—ã¾ã™${NC}"
        VLLM_CMD=$(echo "$VLLM_CMD" | sed "s/--max-model-len $MAX_MODEL_LEN/--max-model-len 8192/")
    fi
fi

echo -e "\n${BLUE}ðŸš€ vLLM ã‚µãƒ¼ãƒãƒ¼èµ·å‹•${NC}"
echo -e "${YELLOW}ã‚³ãƒžãƒ³ãƒ‰:${NC} $VLLM_CMD"
echo ""
echo -e "${GREEN}ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...${NC}"
echo -e "${YELLOW}çµ‚äº†ã™ã‚‹ã«ã¯ Ctrl+C ã‚’æŠ¼ã—ã¦ãã ã•ã„${NC}"
echo ""

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
LOG_FILE="$HOME/.cache/back2task/vllm.log"
mkdir -p "$(dirname "$LOG_FILE")"

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
echo "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: $LOG_FILE"
echo "API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: http://localhost:$PORT"
echo "ãƒ¢ãƒ‡ãƒ«å: gpt-oss-20b"
echo ""

# tmux ã‚»ãƒƒã‚·ãƒ§ãƒ³èµ·å‹•ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
if command -v tmux &> /dev/null; then
    echo -e "${BLUE}ðŸ’¡ tmux ã‚»ãƒƒã‚·ãƒ§ãƒ³ 'back2task-llm' ã§èµ·å‹•ã—ã¾ã™${NC}"
    echo "   ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚¢ã‚¿ãƒƒãƒ: tmux attach -t back2task-llm"
    echo "   ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§: tmux list-sessions"
    echo ""
    
    tmux new-session -d -s back2task-llm "
        source '$VLLM_ENV/bin/activate'
        echo 'ðŸ¤– Back2Task LLM Server'
        echo 'ãƒ¢ãƒ‡ãƒ«: $MODEL_NAME'
        echo 'ãƒãƒ¼ãƒˆ: $PORT'
        echo 'ãƒ­ã‚°: $LOG_FILE'
        echo ''
        $VLLM_CMD 2>&1 | tee '$LOG_FILE'
    "
    
    echo -e "${GREEN}âœ… vLLM ã‚µãƒ¼ãƒãƒ¼ãŒãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•ã—ã¾ã—ãŸ${NC}"
    echo ""
    echo "æŽ¥ç¶šãƒ†ã‚¹ãƒˆ:"
    echo "  curl http://localhost:$PORT/v1/models"
    echo ""
    echo "Back2Task API ã§ã®ä½¿ç”¨:"
    echo "  LLM_URL=http://localhost:$PORT python api/main.py"
    
else
    # tmux ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç›´æŽ¥èµ·å‹•
    exec $VLLM_CMD 2>&1 | tee "$LOG_FILE"
fi